"""
Comprehensive SSD-NeRF Model Evaluation Script
Evaluates and compares Dynamic vs Static SSD-NeRF models
Enhanced with better error handling and logging
"""

import argparse
import sys
import os
import logging
import traceback

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.evaluation.evaluator import SSDNeRFBenchmark
from src.data.dataset import KITTIDataset
from src.utils.config_utils import load_config

def setup_logging():
    """ë¡œê¹… ì„¤ì •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def validate_inputs(args):
    """ì…ë ¥ íŒŒë¼ë¯¸í„° ê²€ì¦"""
    logger = logging.getLogger(__name__)
    
    # Config íŒŒì¼ í™•ì¸
    if not os.path.exists(args.config):
        logger.error(f"âŒ Config file not found: {args.config}")
        return False
    
    # Checkpoint íŒŒì¼ í™•ì¸
    if args.model_type in ['dynamic', 'both'] and args.dynamic_checkpoint:
        if not os.path.exists(args.dynamic_checkpoint):
            logger.error(f"âŒ Dynamic checkpoint not found: {args.dynamic_checkpoint}")
            return False
    
    if args.model_type in ['static', 'both'] and args.static_checkpoint:
        if not os.path.exists(args.static_checkpoint):
            logger.error(f"âŒ Static checkpoint not found: {args.static_checkpoint}")
            return False
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
    output_dir = os.path.dirname(args.output)
    if output_dir:
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"ğŸ“ Output directory ready: {output_dir}")
    
    return True

def main():
    logger = setup_logging()
    
    parser = argparse.ArgumentParser(
        description="ğŸ” Comprehensive SSD-NeRF Model Evaluation (Enhanced)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ ì‚¬ìš©ë²•:
  # Dynamic ëª¨ë¸ë§Œ í‰ê°€
  python scripts/run_evaluation.py --config configs/default_config.py --dynamic_checkpoint output/checkpoints/dynamic_model.pth --model_type dynamic
  
  # Static ëª¨ë¸ë§Œ í‰ê°€  
  python scripts/run_evaluation.py --config configs/default_config.py --static_checkpoint output/checkpoints/static_model.pth --model_type static
  
  # ë‘ ëª¨ë¸ ë¹„êµ
  python scripts/run_evaluation.py --config configs/default_config.py --dynamic_checkpoint dynamic.pth --static_checkpoint static.pth --model_type both

  # ë‹¤ë¥¸ ë°ì´í„°ì…‹ìœ¼ë¡œ í‰ê°€ (ì¼ë°˜í™” ì„±ëŠ¥ í…ŒìŠ¤íŠ¸)
  python scripts/run_evaluation.py --config configs/default_config.py --dynamic_checkpoint dynamic.pth --model_type dynamic --data_path data/other_datasets/nuscenes_mini
  
  # ì—¬ëŸ¬ í™˜ê²½ì—ì„œ ëª¨ë¸ í‰ê°€
  python scripts/run_evaluation.py --config configs/default_config.py --dynamic_checkpoint dynamic.pth --model_type dynamic --data_path data/other_datasets/cadc_winter --output output/eval_winter.json
  python scripts/run_evaluation.py --config configs/default_config.py --dynamic_checkpoint dynamic.pth --model_type dynamic --data_path data/other_datasets/oxford_robotcar --output output/eval_rainy.json
        """
    )
    parser.add_argument('--config', type=str, required=True, help="Path to config file")
    parser.add_argument('--dynamic_checkpoint', type=str, help="Path to dynamic model checkpoint")
    parser.add_argument('--static_checkpoint', type=str, help="Path to static model checkpoint")
    parser.add_argument('--model_type', type=str, choices=['dynamic', 'static', 'both'], default='both',
                        help="Which model(s) to evaluate")
    parser.add_argument('--max_samples', type=int, default=100, help="Maximum samples to evaluate")
    parser.add_argument('--output', type=str, default='output/evaluation_results.json', 
                        help="Output file for results")
    parser.add_argument('--split', type=str, default='test', choices=['train', 'test'],
                        help="Dataset split to evaluate on")
    parser.add_argument('--use_dummy_data', action='store_true',
                        help="Use dummy data for testing when KITTI dataset is not available")
    parser.add_argument('--data_path', type=str, default=None,
                        help="Override data path from config (useful for testing different datasets)")
    
    args = parser.parse_args()
    
    try:
        # ì…ë ¥ ê²€ì¦
        logger.info("ğŸ” Validating inputs...")
        if not validate_inputs(args):
            return 1
        
        # Load configuration
        logger.info(f"ğŸ“‹ Loading configuration from: {args.config}")
        config = load_config(args.config)
        logger.info("âœ… Configuration loaded successfully")
        
        # ë°ì´í„° ê²½ë¡œ override ì²˜ë¦¬
        if args.data_path:
            logger.info(f"ğŸ”„ Overriding data path: {args.data_path}")
            if not os.path.exists(args.data_path):
                logger.error(f"âŒ Specified data path does not exist: {args.data_path}")
                return 1
            config['data']['path'] = args.data_path
            logger.info(f"âœ… Data path updated to: {config['data']['path']}")
        
        # Load dataset
        logger.info(f"ğŸ“Š Loading dataset from: {config['data']['path']} ({args.split} split)")
        try:
            dataset = KITTIDataset(config, split=args.split, create_dummy_data=args.use_dummy_data)
            logger.info(f"âœ… Dataset loaded: {len(dataset)} samples")
            
            if len(dataset) == 0:
                logger.warning("âš ï¸ Dataset is empty. Please check data path and split.")
                return 1
                
        except FileNotFoundError as e:
            if not args.use_dummy_data:
                logger.error(f"âŒ KITTI ë°ì´í„°ì…‹ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                logger.error("ğŸ’¡ í•´ê²° ë°©ë²•:")
                logger.error("  1. KITTI ë°ì´í„°ì…‹ì„ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ì˜¬ë°”ë¥¸ ê²½ë¡œì— ì„¤ì¹˜")
                logger.error("  2. ë˜ëŠ” --use_dummy_data í”Œë˜ê·¸ë¡œ ë”ë¯¸ ë°ì´í„° ì‚¬ìš©")
                logger.error("     ì˜ˆì‹œ: python scripts/run_evaluation.py --config configs/default_config.py --use_dummy_data --model_type dynamic")
                return 1
            else:
                logger.error(f"âŒ ë”ë¯¸ ë°ì´í„° ìƒì„±ì—ë„ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")
                return 1
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„°ì…‹ ë¡œë“œ ì¤‘ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            logger.error(traceback.format_exc())
            if not args.use_dummy_data:
                logger.error("ğŸ’¡ ë”ë¯¸ ë°ì´í„°ë¡œ ì‹œë„í•´ë³´ì„¸ìš”: --use_dummy_data")
            return 1
        
        # Initialize benchmark
        logger.info("ğŸ”§ Initializing evaluation benchmark...")
        try:
            benchmark = SSDNeRFBenchmark(config, dataset)
            logger.info("âœ… Benchmark initialized")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize benchmark: {e}")
            logger.error(traceback.format_exc())
            return 1
        
        # âœ… ê°œì„ ëœ í‰ê°€ ì‹¤í–‰ ë¡œì§
        results = None
        
        if args.model_type == 'both':
            # ë‘ ëª¨ë¸ ë¹„êµ
            if not args.dynamic_checkpoint or not args.static_checkpoint:
                logger.error("âŒ Both dynamic and static checkpoint paths required for comparison")
                return 1
            
            logger.info("ğŸ¥Š Running comprehensive comparison between Dynamic and Static models")
            try:
                results = benchmark.compare_models(
                    args.dynamic_checkpoint, 
                    args.static_checkpoint, 
                    args.max_samples
                )
            except Exception as e:
                logger.error(f"âŒ Model comparison failed: {e}")
                logger.error(traceback.format_exc())
                return 1
                
        elif args.model_type == 'dynamic':
            # Dynamic ëª¨ë¸ë§Œ í‰ê°€
            if not args.dynamic_checkpoint:
                logger.error("âŒ Dynamic checkpoint path required")
                return 1
            
            logger.info("ğŸš€ Evaluating Dynamic SSD-NeRF model")
            try:
                results = benchmark.evaluate_model(args.dynamic_checkpoint, 'dynamic', args.max_samples)
            except Exception as e:
                logger.error(f"âŒ Dynamic model evaluation failed: {e}")
                logger.error(traceback.format_exc())
                return 1
                
        elif args.model_type == 'static':
            # Static ëª¨ë¸ë§Œ í‰ê°€
            if not args.static_checkpoint:
                logger.error("âŒ Static checkpoint path required")
                return 1
            
            logger.info("ğŸ“· Evaluating Static SSD-NeRF model")
            try:
                results = benchmark.evaluate_model(args.static_checkpoint, 'static', args.max_samples)
            except Exception as e:
                logger.error(f"âŒ Static model evaluation failed: {e}")
                logger.error(traceback.format_exc())
                return 1
        
        # Save results
        if results:
            try:
                benchmark.save_results(args.output)
                logger.info(f"ğŸ’¾ Results saved to: {args.output}")
            except Exception as e:
                logger.error(f"âŒ Failed to save results: {e}")
                logger.error(traceback.format_exc())
                return 1
        
        logger.info("ğŸ‰ Evaluation complete!")
        
        # Print quick summary
        if 'comparison' in benchmark.results and benchmark.results['comparison']:
            comp = benchmark.results['comparison']
            recommendation = comp.get('recommendation', {})
            if recommendation:
                logger.info(f"ğŸ† FINAL RECOMMENDATION: {recommendation.get('recommended_model', 'N/A').upper()}")
                logger.info(f"ğŸ’¡ {recommendation.get('reasoning', 'No reasoning provided')}")
        
        return 0
        
    except KeyboardInterrupt:
        logger.info("ğŸ›‘ Evaluation interrupted by user")
        return 1
    except Exception as e:
        logger.error(f"âŒ Unexpected error during evaluation: {e}")
        logger.error(traceback.format_exc())
        return 1

if __name__ == '__main__':
    exit_code = main()
    sys.exit(exit_code) 