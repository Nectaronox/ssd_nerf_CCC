#!/usr/bin/env python3
"""
ν…μ¤νΈμ© λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„± μ¤ν¬λ¦½νΈ
μ‹¤μ  λ¨λΈ ν›λ ¨ μ—†μ΄ evaluationμ„ ν…μ¤νΈν•  μ μκ² ν•΄μ¤λ‹λ‹¤.
"""

import os
import sys
import torch
import torch.nn as nn
import argparse
import logging

# Add the project root to the Python path
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.models.ssd_nerf import DynamicSSDNeRF
from src.model_arch.ssd_nerf_model import StaticSSDNeRF
from src.utils.config_utils import load_config

def setup_logging():
    """λ΅κΉ… μ„¤μ •"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s'
    )
    return logging.getLogger(__name__)

def create_dummy_dynamic_checkpoint(config, output_path):
    """Dynamic SSD-NeRF λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„±"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("π€ Dynamic SSD-NeRF λ”λ―Έ λ¨λΈ μƒμ„± μ¤‘...")
        
        # λ¨λΈ μƒμ„±
        model = DynamicSSDNeRF(config)
        
        # λ”λ―Έ ν›λ ¨ μƒνƒ μƒμ„±
        dummy_checkpoint = {
            'epoch': 10,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': None,  # ν‰κ°€μ—λ” ν•„μ” μ—†μ
            'scheduler_state_dict': None,
            'loss': 0.5,
            'config': config,
            'model_type': 'dynamic',
            'created_by': 'dummy_checkpoint_generator',
        }
        
        # μ²΄ν¬ν¬μΈνΈ μ €μ¥
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(dummy_checkpoint, output_path)
        
        logger.info(f"β… Dynamic λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„± μ™„λ£: {output_path}")
        logger.info(f"π“ λ¨λΈ νλΌλ―Έν„° μ: {sum(p.numel() for p in model.parameters()):,}")
        
        return True
        
    except Exception as e:
        logger.error(f"β Dynamic μ²΄ν¬ν¬μΈνΈ μƒμ„± μ‹¤ν¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def create_dummy_static_checkpoint(config, output_path):
    """Static SSD-NeRF λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„±"""
    logger = logging.getLogger(__name__)
    
    try:
        logger.info("π“· Static SSD-NeRF λ”λ―Έ λ¨λΈ μƒμ„± μ¤‘...")
        
        # λ¨λΈ μƒμ„±
        num_classes = config.get('model', {}).get('ssd_nerf', {}).get('num_classes', 8)
        model = StaticSSDNeRF(num_classes=num_classes)
        
        # λ”λ―Έ ν›λ ¨ μƒνƒ μƒμ„±
        dummy_checkpoint = {
            'epoch': 10,
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': None,  # ν‰κ°€μ—λ” ν•„μ” μ—†μ
            'scheduler_state_dict': None,
            'loss': 0.3,
            'config': config,
            'model_type': 'static',
            'created_by': 'dummy_checkpoint_generator',
        }
        
        # μ²΄ν¬ν¬μΈνΈ μ €μ¥
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        torch.save(dummy_checkpoint, output_path)
        
        logger.info(f"β… Static λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„± μ™„λ£: {output_path}")
        logger.info(f"π“ λ¨λΈ νλΌλ―Έν„° μ: {sum(p.numel() for p in model.parameters()):,}")
        
        return True
        
    except Exception as e:
        logger.error(f"β Static μ²΄ν¬ν¬μΈνΈ μƒμ„± μ‹¤ν¨: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def main():
    parser = argparse.ArgumentParser(
        description="π­ ν…μ¤νΈμ© λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„±κΈ°",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
μ‚¬μ© μμ‹:
  # κΈ°λ³Έ λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„±
  python scripts/create_dummy_checkpoint.py --config configs/default_config.py

  # Dynamic λ¨λΈλ§
  python scripts/create_dummy_checkpoint.py --config configs/default_config.py --model_type dynamic

  # Static λ¨λΈλ§  
  python scripts/create_dummy_checkpoint.py --config configs/default_config.py --model_type static

  # μ»¤μ¤ν…€ κ²½λ΅λ΅ μƒμ„±
  python scripts/create_dummy_checkpoint.py --config configs/default_config.py --output_dir output/test_checkpoints
        """
    )
    
    parser.add_argument('--config', type=str, required=True,
                        help="μ„¤μ • νμΌ κ²½λ΅")
    parser.add_argument('--model_type', type=str, choices=['dynamic', 'static', 'both'], 
                        default='both',
                        help="μƒμ„±ν•  λ¨λΈ νƒ€μ…")
    parser.add_argument('--output_dir', type=str, default='output/checkpoints',
                        help="μ²΄ν¬ν¬μΈνΈ μ €μ¥ λ””λ ‰ν† λ¦¬")
    
    args = parser.parse_args()
    
    logger = setup_logging()
    
    try:
        # μ„¤μ • λ΅λ“
        logger.info(f"π“‹ μ„¤μ • νμΌ λ΅λ“: {args.config}")
        if not os.path.exists(args.config):
            logger.error(f"β μ„¤μ • νμΌμ„ μ°Ύμ„ μ μ—†μµλ‹λ‹¤: {args.config}")
            return 1
            
        config = load_config(args.config)
        logger.info("β… μ„¤μ • λ΅λ“ μ™„λ£")
        
        # μ¶λ ¥ λ””λ ‰ν† λ¦¬ μƒμ„±
        os.makedirs(args.output_dir, exist_ok=True)
        logger.info(f"π“ μ¶λ ¥ λ””λ ‰ν† λ¦¬: {args.output_dir}")
        
        success_count = 0
        
        # Dynamic λ¨λΈ μƒμ„±
        if args.model_type in ['dynamic', 'both']:
            dynamic_path = os.path.join(args.output_dir, 'dummy_dynamic_model.pth')
            if create_dummy_dynamic_checkpoint(config, dynamic_path):
                success_count += 1
                logger.info(f"π’Ύ Dynamic μ²΄ν¬ν¬μΈνΈ: {dynamic_path}")
        
        # Static λ¨λΈ μƒμ„±
        if args.model_type in ['static', 'both']:
            static_path = os.path.join(args.output_dir, 'dummy_static_model.pth')
            if create_dummy_static_checkpoint(config, static_path):
                success_count += 1
                logger.info(f"π’Ύ Static μ²΄ν¬ν¬μΈνΈ: {static_path}")
        
        if success_count > 0:
            logger.info(f"\nπ‰ λ”λ―Έ μ²΄ν¬ν¬μΈνΈ μƒμ„± μ™„λ£! ({success_count}κ°)")
            logger.info("\nπ’» μ‚¬μ© λ°©λ²•:")
            
            if args.model_type in ['dynamic', 'both']:
                logger.info("# Dynamic λ¨λΈ ν‰κ°€:")
                logger.info(f"python scripts/run_evaluation.py --config {args.config} --dynamic_checkpoint {os.path.join(args.output_dir, 'dummy_dynamic_model.pth')} --model_type dynamic")
            
            if args.model_type in ['static', 'both']:
                logger.info("# Static λ¨λΈ ν‰κ°€:")
                logger.info(f"python scripts/run_evaluation.py --config {args.config} --static_checkpoint {os.path.join(args.output_dir, 'dummy_static_model.pth')} --model_type static")
            
            if args.model_type == 'both':
                logger.info("# λ‘ λ¨λΈ λΉ„κµ:")
                logger.info(f"python scripts/run_evaluation.py --config {args.config} --dynamic_checkpoint {os.path.join(args.output_dir, 'dummy_dynamic_model.pth')} --static_checkpoint {os.path.join(args.output_dir, 'dummy_static_model.pth')} --model_type both")
            
            logger.info("\nπ λ‹¤μ–‘ν• ν™κ²½μ—μ„ ν…μ¤νΈ:")
            logger.info("python scripts/download_other_datasets.py --dataset nuscenes_mini --num_samples 10")
            logger.info(f"python scripts/run_evaluation.py --config {args.config} --dynamic_checkpoint {os.path.join(args.output_dir, 'dummy_dynamic_model.pth')} --model_type dynamic --data_path data/other_datasets/nuscenes_mini")
            
            return 0
        else:
            logger.error("β λ¨λ“  μ²΄ν¬ν¬μΈνΈ μƒμ„±μ— μ‹¤ν¨ν–μµλ‹λ‹¤.")
            return 1
    
    except Exception as e:
        logger.error(f"β μμƒμΉ λ»ν• μ¤λ¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return 1

if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code) 