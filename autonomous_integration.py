"""
SSD-NeRF Autonomous Vehicle Integration Example
ììœ¨ì£¼í–‰ì°¨ì— SSD-NeRF ëª¨ë¸ì„ í†µí•©í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì£¼ëŠ” ì˜ˆì‹œ
"""

import torch
import numpy as np
import time
from typing import Dict, List, Tuple
from dataclasses import dataclass

# Import our SSD-NeRF models
from src.models.ssd_nerf import SSD_NeRF as DynamicSSDNeRF
from src.model_arch.ssd_nerf_model import SSDNeRF as StaticSSDNeRF
from src.training.renderer import volume_render, sample_along_rays
from src.utils.ray_utils import get_rays

@dataclass
class SensorData:
    """ììœ¨ì£¼í–‰ì°¨ ì„¼ì„œ ë°ì´í„° êµ¬ì¡°"""
    camera_rgb: np.ndarray      # (H, W, 3) RGB ì¹´ë©”ë¼
    lidar_points: np.ndarray    # (N, 3) LiDAR í¬ì¸íŠ¸ í´ë¼ìš°ë“œ
    timestamp: float            # íƒ€ì„ìŠ¤íƒ¬í”„
    vehicle_pose: np.ndarray    # (4, 4) ì°¨ëŸ‰ ìœ„ì¹˜/ìì„¸
    velocity: float             # ì°¨ëŸ‰ ì†ë„

@dataclass  
class DetectionResult:
    """3D ê°ì²´ ê²€ì¶œ ê²°ê³¼"""
    boxes_3d: List[np.ndarray]  # 3D ë°”ìš´ë”© ë°•ìŠ¤ë“¤
    classes: List[str]          # ê°ì²´ í´ë˜ìŠ¤ (Car, Pedestrian, etc.)
    scores: List[float]         # ì‹ ë¢°ë„ ì ìˆ˜
    track_ids: List[int]        # ê°ì²´ ì¶”ì  ID

class AutonomousVehiclePerception:
    """
    ììœ¨ì£¼í–‰ì°¨ìš© SSD-NeRF ì¸ì‹ ì‹œìŠ¤í…œ
    
    ì‹¤ì‹œê°„ìœ¼ë¡œ ì„¼ì„œ ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ì—¬:
    1. 3D í™˜ê²½ ì¬êµ¬ì„±
    2. ë™ì  ê°ì²´ ê²€ì¶œ ë° ì¶”ì 
    3. ì•ˆì „ ìœ„í—˜ í‰ê°€
    4. ê²½ë¡œ ê³„íš ì§€ì›
    """
    
    def __init__(self, config: dict, model_path: str, use_dynamic: bool = True):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.config = config
        self.use_dynamic = use_dynamic
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        if use_dynamic:
            print("ğŸš— Initializing Dynamic SSD-NeRF for autonomous driving")
            self.model = DynamicSSDNeRF(config).to(self.device)
        else:
            print("ğŸš— Initializing Static SSD-NeRF for autonomous driving")
            self.model = StaticSSDNeRF(num_classes=config['model']['ssd_nerf']['num_classes']).to(self.device)
            
        # ëª¨ë¸ ë¡œë“œ
        checkpoint = torch.load(model_path, map_location=self.device)
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model.eval()
        
        # ì¶”ì  ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.object_tracker = {}
        self.next_track_id = 0
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.inference_times = []
        
    def process_sensor_data(self, sensor_data: SensorData) -> Dict:
        """
        ì‹¤ì‹œê°„ ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬
        
        Args:
            sensor_data: ì°¨ëŸ‰ì˜ ì„¼ì„œ ë°ì´í„°
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼ (3D ê²€ì¶œ, í™˜ê²½ ë§µ, ìœ„í—˜ í‰ê°€)
        """
        start_time = time.time()
        
        with torch.no_grad():
            if self.use_dynamic:
                result = self._process_dynamic_model(sensor_data)
            else:
                result = self._process_static_model(sensor_data)
                
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        inference_time = time.time() - start_time
        self.inference_times.append(inference_time)
        
        # ì‹¤ì‹œê°„ ìš”êµ¬ì‚¬í•­ ì²´í¬ (ììœ¨ì£¼í–‰ì€ ë³´í†µ 10Hz ì´ìƒ í•„ìš”)
        if inference_time > 0.1:  # 100ms ì´ˆê³¼ì‹œ ê²½ê³ 
            print(f"âš ï¸  Inference time too slow: {inference_time:.3f}s")
            
        result['inference_time'] = inference_time
        result['fps'] = 1.0 / inference_time
        
        return result
    
    def _process_dynamic_model(self, sensor_data: SensorData) -> Dict:
        """ë™ì  SSD-NeRF ëª¨ë¸ë¡œ ì²˜ë¦¬"""
        
        # ì…ë ¥ ë°ì´í„° ì¤€ë¹„
        rgb_tensor = torch.from_numpy(sensor_data.camera_rgb).permute(2, 0, 1).float() / 255.0
        rgb_tensor = rgb_tensor.unsqueeze(0).to(self.device)
        
        lidar_tensor = torch.from_numpy(sensor_data.lidar_points).float()
        lidar_tensor = lidar_tensor.unsqueeze(0).to(self.device)
        
        # ì‹œê°„ ì •ë³´ (ë™ì  ê°ì²´ ëª¨ë¸ë§ì„ ìœ„í•¨)
        scene_timestep = torch.tensor([sensor_data.timestamp % 1.0]).unsqueeze(0).to(self.device)
        
        # ì¹´ë©”ë¼ íŒŒë¼ë¯¸í„°
        H, W = rgb_tensor.shape[2], rgb_tensor.shape[3]
        focal = 721.5  # KITTI ê¸°ë³¸ê°’ (ì‹¤ì œë¡œëŠ” ìº˜ë¦¬ë¸Œë ˆì´ì…˜ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        c2w = torch.from_numpy(sensor_data.vehicle_pose).float().to(self.device)
        
        # Ray ìƒì„±
        rays_o, rays_d = get_rays(H, W, focal, c2w)
        rays_o, rays_d = rays_o.to(self.device), rays_d.to(self.device)
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬ë¥¼ ìœ„í•œ ray ì„œë¸Œìƒ˜í”Œë§
        num_rays = min(2048, rays_o.shape[0])  # ì„±ëŠ¥ì„ ìœ„í•´ ì œí•œ
        ray_indices = torch.randperm(rays_o.shape[0])[:num_rays]
        rays_o_sample = rays_o[ray_indices]
        rays_d_sample = rays_d[ray_indices]
        
        # 3D í¬ì¸íŠ¸ ìƒ˜í”Œë§
        pts, z_vals = sample_along_rays(rays_o_sample, rays_d_sample, 0.5, 50.0, 64)
        pts = pts.unsqueeze(0)
        view_dirs = rays_d_sample.unsqueeze(0)
        
        # ë””í“¨ì „ íƒ€ì„ìŠ¤í…
        diffusion_timesteps = torch.randint(0, self.config['model']['diffusion']['time_steps'], (1,)).to(self.device)
        
        # ëª¨ë¸ ì¶”ë¡ 
        outputs = self.model(lidar_tensor, view_dirs, pts, diffusion_timesteps, scene_timestep)
        
        # 3D í™˜ê²½ ë Œë”ë§
        raw_output = outputs['nerf_output'].squeeze(0)
        rgb_map, depth_map, acc_map, disp_map = volume_render(raw_output, z_vals, rays_d_sample)
        
        # ë™ì  ê°ì²´ ê²€ì¶œ (displacement ë¶„ì„)
        displacement = outputs['displacement']
        moving_objects = self._detect_moving_objects(displacement, pts, threshold=0.1)
        
        return {
            'detections': moving_objects,
            'rendered_view': rgb_map.cpu().numpy(),
            'depth_map': depth_map.cpu().numpy(),
            'environment_features': outputs['diffusion_features'].cpu().numpy(),
            'dynamic_displacement': displacement.cpu().numpy()
        }
    
    def _process_static_model(self, sensor_data: SensorData) -> Dict:
        """ì •ì  SSD-NeRF ëª¨ë¸ë¡œ ì²˜ë¦¬"""
        
        rgb_tensor = torch.from_numpy(sensor_data.camera_rgb).permute(2, 0, 1).float() / 255.0
        rgb_tensor = rgb_tensor.unsqueeze(0).to(self.device)
        
        # 2D ê²€ì¶œ ìˆ˜í–‰
        locs_2d, confs_2d, pred_3d_params = self.model(rgb_tensor, None)
        
        # ê²€ì¶œ ê²°ê³¼ í›„ì²˜ë¦¬
        detections = self._postprocess_static_detections(locs_2d, confs_2d, pred_3d_params)
        
        return {
            'detections': detections,
            'rendered_view': sensor_data.camera_rgb,  # ì›ë³¸ ì´ë¯¸ì§€
            'depth_map': None,
            'environment_features': None,
            'dynamic_displacement': None
        }
    
    def _detect_moving_objects(self, displacement: torch.Tensor, pts: torch.Tensor, threshold: float = 0.1) -> DetectionResult:
        """ë³€ìœ„ ë¶„ì„ì„ í†µí•œ ë™ì  ê°ì²´ ê²€ì¶œ"""
        
        # ë³€ìœ„ í¬ê¸° ê³„ì‚°
        displacement_magnitude = torch.norm(displacement, dim=-1)
        
        # ì›€ì§ì´ëŠ” í¬ì¸íŠ¸ ë§ˆìŠ¤í¬
        moving_mask = displacement_magnitude > threshold
        
        # í´ëŸ¬ìŠ¤í„°ë§ì„ í†µí•œ ê°ì²´ ë¶„ë¦¬ (ê°„ë‹¨í•œ êµ¬í˜„)
        moving_points = pts.squeeze(0)[moving_mask.squeeze(0)]
        
        if moving_points.shape[0] > 0:
            # ê°„ë‹¨í•œ ë°”ìš´ë”© ë°•ìŠ¤ ìƒì„±
            min_coords = torch.min(moving_points, dim=0)[0]
            max_coords = torch.max(moving_points, dim=0)[0]
            center = (min_coords + max_coords) / 2
            size = max_coords - min_coords
            
            # 3D ë°”ìš´ë”© ë°•ìŠ¤ [x, y, z, l, w, h, yaw]
            bbox_3d = torch.cat([center, size, torch.tensor([0.0], device=self.device)])
            
            return DetectionResult(
                boxes_3d=[bbox_3d.cpu().numpy()],
                classes=['Moving_Object'],
                scores=[0.8],
                track_ids=[self._get_track_id()]
            )
        else:
            return DetectionResult([], [], [], [])
    
    def _postprocess_static_detections(self, locs_2d, confs_2d, pred_3d_params) -> DetectionResult:
        """ì •ì  ëª¨ë¸ ê²€ì¶œ ê²°ê³¼ í›„ì²˜ë¦¬"""
        
        # ê°„ë‹¨í•œ í›„ì²˜ë¦¬ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ NMS ë“±ì´ í•„ìš”)
        if pred_3d_params is not None:
            boxes_3d = pred_3d_params.cpu().numpy()
            scores = torch.softmax(confs_2d, dim=-1).max(dim=-1)[0].cpu().numpy()
            classes = ['Vehicle'] * len(boxes_3d)
            track_ids = [self._get_track_id() for _ in range(len(boxes_3d))]
            
            return DetectionResult(
                boxes_3d=boxes_3d.tolist(),
                classes=classes,
                scores=scores.tolist(),
                track_ids=track_ids
            )
        else:
            return DetectionResult([], [], [], [])
    
    def _get_track_id(self) -> int:
        """ìƒˆë¡œìš´ ì¶”ì  ID ìƒì„±"""
        track_id = self.next_track_id
        self.next_track_id += 1
        return track_id
    
    def get_performance_stats(self) -> Dict:
        """ì„±ëŠ¥ í†µê³„ ë°˜í™˜"""
        if not self.inference_times:
            return {}
            
        return {
            'avg_inference_time': np.mean(self.inference_times),
            'max_inference_time': np.max(self.inference_times),
            'min_inference_time': np.min(self.inference_times),
            'avg_fps': 1.0 / np.mean(self.inference_times),
            'total_frames': len(self.inference_times)
        }

# ì‹¤ì œ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ í†µí•© ì˜ˆì‹œ
def autonomous_driving_integration_example():
    """
    ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ í†µí•© ì˜ˆì‹œ
    """
    
    # ì„¤ì • ë¡œë“œ
    from src.utils.config_utils import load_config
    config = load_config('configs/default_config.py')
    
    # ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    perception_system = AutonomousVehiclePerception(
        config=config,
        model_path='output/checkpoints/model_epoch_100.pth',
        use_dynamic=True
    )
    
    print("ğŸš— ììœ¨ì£¼í–‰ ì¸ì‹ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    print("ğŸ“¡ ì„¼ì„œ ë°ì´í„° ì²˜ë¦¬ ì‹œì‘...")
    
    # ì‹œë®¬ë ˆì´ì…˜ëœ ì„¼ì„œ ë°ì´í„°
    for frame in range(10):
        # ê°€ìƒì˜ ì„¼ì„œ ë°ì´í„° ìƒì„±
        sensor_data = SensorData(
            camera_rgb=np.random.randint(0, 255, (375, 1242, 3), dtype=np.uint8),
            lidar_points=np.random.randn(8192, 3).astype(np.float32),
            timestamp=frame * 0.1,  # 10Hz
            vehicle_pose=np.eye(4, dtype=np.float32),
            velocity=50.0  # km/h
        )
        
        # ì‹¤ì‹œê°„ ì²˜ë¦¬
        result = perception_system.process_sensor_data(sensor_data)
        
        print(f"Frame {frame}: {result['fps']:.1f} FPS, "
              f"{len(result['detections'].boxes_3d)} objects detected")
    
    # ì„±ëŠ¥ í†µê³„
    stats = perception_system.get_performance_stats()
    print("\nğŸ“Š ì„±ëŠ¥ í†µê³„:")
    for key, value in stats.items():
        print(f"   {key}: {value:.3f}")

if __name__ == "__main__":
    autonomous_driving_integration_example() 